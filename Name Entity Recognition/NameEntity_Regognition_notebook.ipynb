{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4YSvpD3A1BMQ"
      },
      "outputs": [],
      "source": [
        "# Import need libraries\n",
        "import re\n",
        "import os, sys\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lHyhIIF0ofT1"
      },
      "outputs": [],
      "source": [
        "# Entity Cleaner: Unites entity tags and fixs misspellings \n",
        "def tags_cleaner(entity):\n",
        "  entity = re.sub('\\n','',entity) # Remove the newline (\\n)\n",
        "  if entity in ['B-LOC', 'B-MIS', 'B-ORG','B-PER','I-LOC','I-MIS','I-ORG','I-PER','O']:\n",
        "    return entity\n",
        "  elif entity in ['B-MIS0','B-MIS1', 'B-MIS2', 'B-MIS3', 'B-MIS-1','B-MIS-2', 'B-MIS1`', 'B-MISS1']:\n",
        "    return 'B-MIS'\n",
        "  elif entity in ['I-MIS0','I-MIS1', 'I-MIS2', 'I-MIS3']:\n",
        "    return 'I-MIS'\n",
        "  elif entity in ['B-ENGLISH', 'B-SPANISH', 'OO', 'IO']:\n",
        "    return 'O'\n",
        "  elif entity == 'I--ORG':\n",
        "    return 'I-ORG'\n",
        "  else:\n",
        "    print('Error with entity:', entity)\n",
        "\n",
        "\n",
        "# Clean/Normalize Arabic Text\n",
        "def clean_str(text):\n",
        "    search = [\"أ\",\"إ\",\"آ\",\"ة\",\"_\",\"-\",\"/\",\".\",\"،\",\" و \",\" يا \",'\"',\"ـ\",\"'\",\"ى\",\"\\\\\",'\\n', '\\t','&quot;','?','؟','!']\n",
        "    replace = [\"ا\",\"ا\",\"ا\",\"ه\",\" \",\" \",\"\",\"\",\"\",\" و\",\" يا\",\"\",\"\",\"\",\"ي\",\"\",' ', ' ',' ',' ? ',' ؟ ',' ! ']\n",
        "    \n",
        "    # Remove tashkeel\n",
        "    p_tashkeel = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n",
        "    text = re.sub(p_tashkeel,\"\", text)\n",
        "    \n",
        "    # Remove longation\n",
        "    p_longation = re.compile(r'(.)\\1+')\n",
        "    subst = r\"\\1\\1\"\n",
        "    text = re.sub(p_longation, subst, text)\n",
        "    \n",
        "    text = text.replace('وو', 'و')\n",
        "    text = text.replace('يي', 'ي')\n",
        "    text = text.replace('اا', 'ا')\n",
        "    \n",
        "    for i in range(0, len(search)):\n",
        "        text = text.replace(search[i], replace[i])\n",
        "    \n",
        "    # Trim    \n",
        "    text = text.strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Remove empty strings or strings that contains spaces only from sentences\n",
        "def re_clean(old_sentence, old_tags):\n",
        "  space_regex = re.compile(\"\\s+\")\n",
        "  new_sentence = []\n",
        "  new_tags = []\n",
        "  for j in range(len(old_sentence)):\n",
        "    # add word if not empty and doesn't contain spaces only\n",
        "    if old_sentence[j]!=\"\" and space_regex.match(old_sentence[j])==None:\n",
        "      new_sentence.append(old_sentence[j])\n",
        "      new_tags.append(old_tags[j])\n",
        "  \n",
        "  return new_sentence, new_tags\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uInDRuaERS4K",
        "outputId": "93722e89-e033-426c-930b-e0c3e1b588ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing: Atom.txt\n",
            "Processing: Christiano_Ronaldo.txt\n",
            "Processing: Computer.txt\n",
            "Processing: Computer_Software.txt\n",
            "Processing: Crusades.txt\n",
            "Processing: Damascus.txt\n",
            "Processing: Enrico_Fermi.txt\n",
            "Processing: Football.txt\n",
            "Processing: Ibn_Tolun_Mosque.txt\n",
            "Processing: Imam_Hussein_Shrine.txt\n",
            "Processing: Internet.txt\n",
            "Processing: Islamic_Golden_Age.txt\n",
            "Processing: Islamic_History.txt\n",
            "Processing: Light.txt\n",
            "Processing: Linux.txt\n",
            "Processing: Nuclear_Power.txt\n",
            "Processing: Periodic_Table.txt\n",
            "Processing: Physics.txt\n",
            "Processing: Portugal_football_team.txt\n",
            "Processing: Raul_Gonzales.txt\n",
            "Processing: Razi.txt\n",
            "Processing: Real_Madrid.txt\n",
            "Processing: Richard_Stallman.txt\n",
            "Processing: Soccer_Worldcup.txt\n",
            "Processing: Solaris.txt\n",
            "Processing: Summer_Olympics2004.txt\n",
            "Processing: Ummaya_Mosque.txt\n",
            "Processing: X_window_system.txt\n",
            "Done [Sentences: 2687 , Tags: 2687 , Unique Words: 17481\n"
          ]
        }
      ],
      "source": [
        "# Read sentences\n",
        "sentences = [] \n",
        "tags = []\n",
        "vocab = set()\n",
        "\n",
        "dir_path = os.path.join(sys.path[0])\n",
        "\n",
        "corpus_path = os.path.join(dir_path, \"AQMAR_Arabic_NER_corpus-1.0\")\n",
        "for file in os.listdir(corpus_path):\n",
        "  if file.endswith('.txt'): # Get txt files only\n",
        "    print('Processing:', file)\n",
        "    topic = open(os.path.join(corpus_path,file))\n",
        "    sentence = []\n",
        "    entity = []\n",
        "    for line in topic.readlines():\n",
        "      if line == '\\n': # Sentence end\n",
        "        recleaned = re_clean(sentence, entity)\n",
        "        sentences.append(recleaned[0].copy())\n",
        "        tags.append(recleaned[1].copy())\n",
        "        sentence.clear()\n",
        "        entity.clear()\n",
        "      else:\n",
        "        line = line.split(sep=' ')\n",
        "        clean_word = clean_str(line[0])       # Cleaning word\n",
        "        vocab.add(clean_word)                 # Add word to the vocab\n",
        "        sentence.append(clean_word)           # Add the word\n",
        "        entity.append(tags_cleaner(line[1]))  # Clean and add entity\n",
        "\n",
        "\n",
        "print('Done [Sentences:', len(sentences), ', Tags:', len(tags), ', Unique Words:', len(vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "T4EKWDM599h9"
      },
      "outputs": [],
      "source": [
        "# Make a mapping betwween words and their IDs\n",
        "word2id = {word:id for  id, word in enumerate(vocab)}\n",
        "id2word = {id:word for  id, word in enumerate(vocab)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-NX05ZOQnmk"
      },
      "source": [
        "**The unbalanced dataset problem**\n",
        "\n",
        "I found that:\n",
        "- max length of sentence is 290\n",
        "- 2156 out of 2687 sentences have 40 words or less (80.2% of the data)\n",
        "- 2485 out of 2687 sentences have 60 words or less (92.2% of the data)\n",
        "- Sentences have empty strings/words due to cleaning and were tagged 'O'\n",
        "- The 'O' tag represents 87.3% of the words\n",
        "\n",
        "----\n",
        "I re-cleaned the sentences again and removed all empty words and strings that contain spaces only. It made slight difference but it wasn't enough.\n",
        "\n",
        "Results:\n",
        "- max length of sentence is 271\n",
        "- 2240 out of 2687 sentences have 40 words or less (83.4% of the data)\n",
        "- 2514 out of 2687 sentences have 60 words or less (93.5% of the data)\n",
        "- No empty strings\n",
        "- The 'O' tag represents 86.4% of the words\n",
        "\n",
        "----\n",
        "\n",
        "I tried to find the ratio of tags according to sentence size. We made bins of size 20 words from 0 to 160 (8 bins)\n",
        "```\n",
        "# Percent of each tag per bin\n",
        "'B-LOC': [1.5, 2.1, 2.2, 2.9, 2.4, 2.4, 3.6, 1.6]\n",
        "'B-MIS': [5.2, 3.9, 3.5, 1.9, 1.6, 2.0, 2.2, 0.6]\n",
        "'B-ORG': [0.6, 0.6, 0.4, 0.9, 1.1, 1.6, 1.6, 1.6]\n",
        "'B-PER': [1.8, 1.9, 1.7, 3.1, 3.7, 4.2, 3.4, 4.7]\n",
        "'I-LOC': [0.6, 0.9, 1.1, 1.4, 0.8, 0.8, 1.9, 0.2]\n",
        "'I-MIS': [2.2, 2.2, 2.1, 1.3, 1.6, 2.1, 1.7, 0.6]\n",
        "'I-ORG': [0.7, 0.7, 0.5, 0.9, 0.9, 1.0, 1.7, 1.3]\n",
        "'I-PER': [0.9, 1.2, 1.0, 2.1, 2.5, 2.5, 2.0, 7.8]\n",
        "'O': [86.6, 86.6, 87.4, 85.5, 85.4, 83.5, 82.0, 81.5]\n",
        "```\n",
        "\n",
        "As it can be seen, all bins have the same distribution\n",
        "\n",
        "----\n",
        "Next step is to choose another padding size, I think size of 40 would be best as most sentences are 40 words or less."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9B0jdBD9-a9o"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Sentence encoder\n",
        "def encode_sentence(old_sentence):\n",
        "  encoded_sentence = []\n",
        "  for word in old_sentence:\n",
        "    try:\n",
        "      encoded_sentence.append(word2id[word])\n",
        "    except KeyError:\n",
        "      encoded_sentence.append(0) # A dummy digit for out of vocab\n",
        "\n",
        "  return encoded_sentence\n",
        "\n",
        "# Encode Tags\n",
        "tags_encoding = {\n",
        "    'B-LOC':0,\n",
        "    'B-MIS':1,\n",
        "    'B-ORG':2,\n",
        "    'B-PER':3,\n",
        "    'I-LOC':4,\n",
        "    'I-MIS':5,\n",
        "    'I-ORG':6,\n",
        "    'I-PER':7,\n",
        "    'O':8\n",
        "  }\n",
        "def encode_tags(old_tags):\n",
        "  new_tags = [tags_encoding[tag] for tag in old_tags]\n",
        "  new_tags = to_categorical(y = new_tags, num_classes=9)\n",
        "  return new_tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8p6QS6azAFp3"
      },
      "outputs": [],
      "source": [
        "# Encoding\n",
        "sentences_encoded = []\n",
        "tags_encoded = []\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "  sentences_encoded.append(encode_sentence(sentences[i]))\n",
        "  tags_encoded.append(encode_tags(tags[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kKcTVg3uB84a"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Padding\n",
        "MAX_SEQUENCE_LENGTH = 40\n",
        "\n",
        "sentences_padded = pad_sequences(sequences = sentences_encoded, \n",
        "                                 maxlen=MAX_SEQUENCE_LENGTH,\n",
        "                                 dtype='int32', \n",
        "                                 padding='post',\n",
        "                                 truncating='post',\n",
        "                                 value = 0)\n",
        "tags_padded = pad_sequences(sequences = tags_encoded, \n",
        "                                 maxlen=MAX_SEQUENCE_LENGTH,\n",
        "                                 dtype='int32', \n",
        "                                 padding='post',\n",
        "                                 truncating='post',\n",
        "                                 value = np.array([0., 0., 0., 0., 0., 0., 0., 0., 1.]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "n3io8aspRJTK"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Splitting data\n",
        "train_sentences, test_sentences, train_labels, test_labels = train_test_split(sentences_padded, \n",
        "                                                                              tags_padded, \n",
        "                                                                              train_size=0.8, \n",
        "                                                                              random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaMr0gIxvpus",
        "outputId": "c48a6606-2441-4dd3-d802-519f9f83ee95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "لمحمد 0.7260121703147888\n",
            "احمد 0.7142193913459778\n",
            "عبدالرحمن 0.6745273470878601\n",
            "ابراهيم 0.6723851561546326\n",
            "مهدي 0.6686975955963135\n",
            "محمود 0.664846658706665\n",
            "يحي 0.637116551399231\n",
            "اسماعيل 0.6307213306427002\n",
            "حموده 0.6287057995796204\n",
            "عبدالحميد 0.6267551183700562\n"
          ]
        }
      ],
      "source": [
        "import gensim\n",
        "\n",
        "# Load the Word2Vec model\n",
        "weights_path = os.path.join(dir_path, \"wiki_cbow_300\\wikipedia_cbow_300\")\n",
        "araVec = gensim.models.Word2Vec.load(weights_path)\n",
        "\n",
        "# Testing\n",
        "most_similar = araVec.wv.most_similar( \"محمد\" )\n",
        "for term, score in most_similar:\n",
        "\tprint(term, score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtwcA63dOEYR",
        "outputId": "cafd9372-0e11-408b-8531-4e72ed8af874"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(17481, 300)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create an embedding matrix for the embedding layer\n",
        "num_words = len(vocab)\n",
        "# embed_size, = araVec['محمود'].shape\n",
        "embed_size = araVec.wv.get_vector('محمود').shape[0]\n",
        "# print(embed_size[0])\n",
        "embedding_matrix = np.zeros(shape=(num_words, embed_size))\n",
        "\n",
        "for word, id in word2id.items():\n",
        "  try:\n",
        "    embedding_matrix[id] = araVec.wv.get_vector(word)\n",
        "  except KeyError:\n",
        "    embedding_matrix[id] = np.zeros(embed_size)\n",
        "\n",
        "embedding_matrix.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTtLQW_3Npb4",
        "outputId": "fc849f1e-3e41-4106-8789-dab80ec63ca0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 40, 300)           5244300   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 40, 300)           721200    \n",
            "                                                                 \n",
            " time_distributed (TimeDistr  (None, 40, 9)            2709      \n",
            " ibuted)                                                         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,968,209\n",
            "Trainable params: 723,909\n",
            "Non-trainable params: 5,244,300\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ahmed\\miniconda3\\envs\\test_env\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import LSTM, Input, Dense, Embedding, TimeDistributed\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "\n",
        "tf.keras.backend.clear_session() # Makes sure old model was deleted if exists\n",
        "\n",
        "lstm_model = Sequential()\n",
        "# Adding Layers\n",
        "lstm_model.add(Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32'))\n",
        "lstm_model.add(Embedding(input_dim = len(vocab),              # Vocabulary Size (number of unique words for training)\n",
        "                        output_dim = embed_size,              # Length of the vector for each word (embedding dimension)\n",
        "                        input_length = MAX_SEQUENCE_LENGTH,   # Maximum length of a sequence\n",
        "                        weights = [embedding_matrix],         # Send the needed AraVec Weights\n",
        "                        trainable = False))\n",
        "\n",
        "lstm_model.add(LSTM(units = embed_size, \n",
        "                    return_sequences=True,\n",
        "                    dropout=0.5, \n",
        "                    recurrent_dropout=0.5))\n",
        "lstm_model.add(TimeDistributed(Dense(9, activation='softmax')))\n",
        "\n",
        "# Compile the model\n",
        "lstm_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999), \n",
        "                   loss='categorical_crossentropy',\n",
        "                   metrics=['accuracy'])\n",
        "lstm_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmUcYDjbVVP-",
        "outputId": "762944b0-8338-4c15-8336-df40e9bbaf3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "183/183 [==============================] - 45s 224ms/step - loss: 0.5834 - accuracy: 0.8790 - val_loss: 0.3570 - val_accuracy: 0.9247\n",
            "Epoch 2/10\n",
            "183/183 [==============================] - 41s 222ms/step - loss: 0.3426 - accuracy: 0.9225 - val_loss: 0.2800 - val_accuracy: 0.9283\n",
            "Epoch 3/10\n",
            "183/183 [==============================] - 41s 222ms/step - loss: 0.2867 - accuracy: 0.9272 - val_loss: 0.2396 - val_accuracy: 0.9342\n",
            "Epoch 4/10\n",
            "183/183 [==============================] - 41s 223ms/step - loss: 0.2545 - accuracy: 0.9311 - val_loss: 0.2135 - val_accuracy: 0.9379\n",
            "Epoch 5/10\n",
            "183/183 [==============================] - 41s 222ms/step - loss: 0.2299 - accuracy: 0.9348 - val_loss: 0.1955 - val_accuracy: 0.9426\n",
            "Epoch 6/10\n",
            "183/183 [==============================] - 41s 224ms/step - loss: 0.2132 - accuracy: 0.9374 - val_loss: 0.1837 - val_accuracy: 0.9455\n",
            "Epoch 7/10\n",
            "183/183 [==============================] - 56s 307ms/step - loss: 0.1994 - accuracy: 0.9407 - val_loss: 0.1773 - val_accuracy: 0.9476\n",
            "Epoch 8/10\n",
            "183/183 [==============================] - 54s 295ms/step - loss: 0.1880 - accuracy: 0.9434 - val_loss: 0.1643 - val_accuracy: 0.9500\n",
            "Epoch 9/10\n",
            "183/183 [==============================] - 54s 297ms/step - loss: 0.1800 - accuracy: 0.9460 - val_loss: 0.1588 - val_accuracy: 0.9513\n",
            "Epoch 10/10\n",
            "183/183 [==============================] - 54s 294ms/step - loss: 0.1714 - accuracy: 0.9475 - val_loss: 0.1539 - val_accuracy: 0.9524\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x1d6b8651ed0>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lstm_model.fit(train_sentences, \n",
        "               train_labels, \n",
        "               validation_split=0.15, \n",
        "               batch_size = 10,\n",
        "               epochs = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiB21M7bY7bW",
        "outputId": "9409c6fc-6c44-4c0f-fa4d-918bdfedabda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "17/17 [==============================] - 1s 62ms/step - loss: 0.1717 - accuracy: 0.9469\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.17171882092952728, 0.9469330906867981]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lstm_model.evaluate(test_sentences, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "LRFJTTFCWir0"
      },
      "outputs": [],
      "source": [
        "def lstm_predict(sentence:str):\n",
        "  sentence = sentence.split(sep=' ')\n",
        "  # Keeping track of words so not to process 40 words every time\n",
        "  word_count = len(sentence) \n",
        "  # Clean sentence\n",
        "  ready_sentence = [clean_str(word) for word in sentence]\n",
        "  # Encode sentence\n",
        "  ready_sentence = encode_sentence(ready_sentence)\n",
        "  # Padding sentence\n",
        "  ready_sentence = pad_sequences(sequences = [ready_sentence], \n",
        "                                 maxlen=MAX_SEQUENCE_LENGTH,\n",
        "                                 dtype='int32', \n",
        "                                 padding='post',\n",
        "                                 truncating='post',\n",
        "                                 value = 0)\n",
        "  \n",
        "  # Predict and return actual words only\n",
        "  predictions = lstm_model.predict(ready_sentence)[0][0:word_count]\n",
        "\n",
        "  i = 0\n",
        "  for prediction in predictions:\n",
        "    tags_onehot = {\n",
        "      'B-LOC':np.array([1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
        "      'B-MIS':np.array([0., 1., 0., 0., 0., 0., 0., 0., 0.]),\n",
        "      'B-ORG':np.array([0., 0., 1., 0., 0., 0., 0., 0., 0.]),\n",
        "      'B-PER':np.array([0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
        "      'I-LOC':np.array([0., 0., 0., 0., 1., 0., 0., 0., 0.]),\n",
        "      'I-MIS':np.array([0., 0., 0., 0., 0., 1., 0., 0., 0.]),\n",
        "      'I-ORG':np.array([0., 0., 0., 0., 0., 0., 1., 0., 0.]),\n",
        "      'I-PER':np.array([0., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
        "      'O':np.array([0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
        "    }\n",
        "    tags_scores = {\n",
        "      'B-LOC':0,\n",
        "      'B-MIS':0,\n",
        "      'B-ORG':0,\n",
        "      'B-PER':0,\n",
        "      'I-LOC':0,\n",
        "      'I-MIS':0,\n",
        "      'I-ORG':0,\n",
        "      'I-PER':0,\n",
        "      'O':0\n",
        "    }\n",
        "    for tag in list(tags_onehot.keys()):\n",
        "      tags_scores[tag] = np.linalg.norm(tags_onehot[tag] - prediction)\n",
        "    \n",
        "    \n",
        "    print(sentence[i],':',min(tags_scores, key=tags_scores.get))\n",
        "    i+=1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "أحمد : B-PER\n",
            "خالد : I-PER\n",
            "محمد : I-PER\n",
            "كامل : I-PER\n",
            "أكل : O\n",
            "كامل : O\n",
            "الطعام : O\n"
          ]
        }
      ],
      "source": [
        "x = 'أحمد خالد محمد كامل أكل كامل الطعام'\n",
        "lstm_predict(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "# save the model to disk\n",
        "filename = 'finalized_model.pkl'\n",
        "joblib.dump(lstm_model, filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "NLP_Task3_playground.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
